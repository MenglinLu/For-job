1. LR和SVM的对比
1) LR采用logit loss, SVM采用hinge loss。 对应的损失函数分别为
logit loss: L(theta)=sigma(y*log(h-theta(x))+(1-y)log(1-h-theta(x)))
hinge loss: L(alpha)=w^2/2+sigma(alpha*(1-(y*(wx+b))) SVM基于几何间隔最大化
2) LR使用所有的数据点，对异常值敏感 SVM仅使用支持向量即少部分数据点 对异常值不敏感 SVM改变非支持向量不会引起超平面的改变
3） 计算复杂度不同。 对于海量数据， LR计算复杂度低，SVM复杂度高  当数据量及特征较少时，LR和SVM都很快，SVM效率更高，但准确率LR更好。样本量稍微增加之后，SVM时间复杂度变高但准确率也优于LR. 当数据量增长到20000时，特征维数增长到200时，SVM的运行时间剧烈增加，远远超过了LR的运行时间。但是准确率却和LR相差无几。
4） 对非线性问题的处理方式不同。LR在处理非线性问题时主要通过组合交叉特征来解决，SVM不但可以通过这种方法也可以通过kernel方式将低维线性不可分数据映射到高维线性可分数据。常采用的核包括高斯核、拉普拉斯核等。
5） LR是参数模型，SVM是非参数模型。
6） LR需要正则化，SVM不需要正则化，因为损失函数中已经包括了正则项。这也是为什么SVM结构风险小的原因。
